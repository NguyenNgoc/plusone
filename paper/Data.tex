% XeLaTeX can use any Mac OS X font. See the setromanfont command below.
% Input to XeLaTeX is full Unicode, so Unicode characters can be typed directly into the source.

% The next lines tell TeXShop to typeset with xelatex, and to open and save the source with Unicode encoding.

%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

%\documentclass[twocolumn, 10pt]{article}
\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{lastpage}
\pagestyle{fancy}
\cfoot{\thepage\ of \pageref{LastPage}}
% Will Robertson's fontspec.sty can be used to simplify font choices.
% To experiment, open /Applications/Font Book to examine the fonts provided on Mac OS X,
% and change "Hoefler Text" to any of these choices.
\usepackage{slashbox} % for table slashbox
%%%%%%%%%%%%%%%% costomed package
\usepackage{listings}\lstset{language = bash}
\usepackage{framed}
\usepackage{setspace}%for double space
%\doublespacing%for double space
%\usepackage{listings}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

%%%%%%%%%%%%%%%%
%\usepackage{fontspec,xltxtra,xunicode}
%\defaultfontfeatures{Mapping=tex-text}
%\setromanfont[Mapping=tex-text]{Hoefler Text}
%\setsansfont[Scale=MatchLowercase,Mapping=tex-text]{Gill Sans}
%\setmonofont[Scale=MatchLowercase]{Andale Mono}

\title{Synthetic Data Generation from Topic Models }
\author{}
\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
\section{Probabilistic latent semantic indexing (PLSI)}
PLSI is a statistical latent class model first proposed by Thomas Hofman in \cite{?}. It assumes that a document corpus is generated from a small number of latent topics. Each topic has its own distribution over the terms, and each document has a distribution over topics. More precisely, suppose there are $n$ documents, $m$ terms, and $k$ underlying topics. The model assumes that there exist a $n\cross k$ matrix $U$ where each row is a document's distribution over the topics, and a $k\cross m$ matrix $V$ where each row is the distribution over terms for a topic. Write $A=UV$, then the $i$th document is generated by first picking a document length $d$, and sampling each word from a multinomial distribution given by the $i$th row of $A$. 

We implemented a data generator according to the PLSI model, and tested the performance of various algorithms on the synthetic data. The generator takes in parameters
\begin{enumerate}
	\item T, the number of topics
	\item K, the size of the corpus
	\item P, the average number of words per document (drawn from a Poisson distribution)
	\item V, the size of our vocabulary
	\item N, the amount of noise per document
\end{enumerate}

We assume a fixed distribution over words per topic D_wt, represented as a T x V matrix. For each row, we uniformly sampling on the interval [0,1) V times and normalize.

Each document is generated as follows:
1. Determine the number of words in the document, p. From p, we choose a portion of them, n, to be chosen uniformly at random from V.
p ~ Poisson(P)
n ~ Poisson(N)

2. Generate a distribution over topics, D_t.
uniformly sample from the interval [0,1) T times, then normalize.

3. Generate p - n words:
first sample from t ~ D_t, then sample w ~ D_wt (from the t^th row of D_wt)

4. Add noise
from V, uniformly at random pick n words 

\end{document}
